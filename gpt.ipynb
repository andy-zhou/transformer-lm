{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605aceae-64b9-4c07-96fc-9246628b4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import data, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1402fb8-acf8-4c72-943a-9b0efbc3b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "encoder, train, val = data.load(\"data/tinyshakespeare.txt\")\n",
    "print(encoder.decode(train[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063c075",
   "metadata": {},
   "source": [
    "## Baseline: Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "054d06c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline Model...\n",
      "\n",
      "Iteration      0/10000: train_loss=4.2492, val_loss=4.2379\n",
      "Iteration   2000/10000: train_loss=2.4981, val_loss=2.5174\n",
      "Iteration   4000/10000: train_loss=2.4844, val_loss=2.5006\n",
      "Iteration   6000/10000: train_loss=2.4780, val_loss=2.4929\n",
      "Iteration   8000/10000: train_loss=2.4707, val_loss=2.4926\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Baseline Model...\\n\")\n",
    "baseline_model = models.train_bigram_model(train, val, encoder)\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1a84bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Baseline Generation\n",
      "---------\n",
      "\n",
      "\n",
      "TOMourime shat los theng cizlouly bre\n",
      "\n",
      "\n",
      "An I h w'Tho he.\n",
      "S:\n",
      "Lough hind\n",
      "m.\n",
      "\n",
      "Havit d al\n",
      "Thom ade.\n",
      "NGfowhix'shson t.\n",
      "INCHend ke the ancalim He willinourte.\n",
      "I Gokisean tofere t inge nkesoffomanuton s istoule styomito ord, y o phire nd acelior, corethe f dlllimireve adlllo, od d mug t l, wilor te meal the ery ouson llsis I s s rrurfehieead Hanghero, wis;\n",
      "Tholeresotr'stowh onou, helsing henlencoushame\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Baseline Generation\\n---------\")\n",
    "print(encoder.decode(baseline_model.generate(num_characters=400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168efbd8",
   "metadata": {},
   "source": [
    "## Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8ee9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer Model...\n",
      "\n",
      "Iteration      0/10000: train_loss=4.3390, val_loss=4.3433\n",
      "Iteration   2000/10000: train_loss=2.1113, val_loss=2.1450\n",
      "Iteration   4000/10000: train_loss=1.9170, val_loss=2.0008\n",
      "Iteration   6000/10000: train_loss=1.8106, val_loss=1.9211\n",
      "Iteration   8000/10000: train_loss=1.7398, val_loss=1.8828\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Transformer Model...\\n\")\n",
    "transformer_model = models.train_transformer_model(\n",
    "    train, val, encoder, lr=3e-4, layers=3\n",
    ")\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2faa81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Transformer Generation\n",
      "---------\n",
      "To quo rody too behame, wease; how a\n",
      "Arainsue you. An, this me my winks ride conce plove-bestipes us he is a twese toil I'll you like heath: whoo do thank of his the gonest's we.\n",
      "\n",
      "MENESTER:\n",
      "Anch, whall that?\n",
      "Ming thou arest to let it; and the dischmourn, lord, it Preatde, where slee.\n",
      "\n",
      "GROLETH:\n",
      "Your prorve us, for no, twell to know\n",
      "We quarl make nemanle:--ty go out that greation to hate uncrew,\n",
      "Sry land, enoun it doe, no that e.\n",
      "\n",
      "HENSAR RI:\n",
      "Which bere his offors, hamse\n",
      "Death; thoust she\n",
      "king to resticill\n",
      "Dike a draie, I renike and Vurt more; i dish\n",
      "she our four likes, let most for a keam,\n",
      "And till hand very all moth she will!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I'll tenver hand and he\n",
      "me noble of myben; gone.t to me,\n",
      "Thereforb! Why, state, Gain the took was my fell bleath my brother blook,\n",
      "Four sin\n",
      "For comest made.\n",
      "\n",
      "MARWICINIUS:\n",
      "No, wheve Richard?\n",
      "\n",
      "ARGHORSENEW:\n",
      "So I have bear, are and to my crauge my drobjes, thereign,\n",
      "That ursest cany healt, bear a makel offus tie have\n",
      "And in of all grace, and wink your \n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Transformer Generation\\n---------\")\n",
    "print(encoder.decode(transformer_model.generate(num_characters=1000)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
