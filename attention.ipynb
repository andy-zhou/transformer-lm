{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Attention Mechanisms\n",
    "\n",
    "Some simple pytorch manipulation based on the math in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "B, T, C = 4, 8, 32\n",
    "retrieval_dims = 16\n",
    "value_dims = 16\n",
    "\n",
    "# Input\n",
    "x = torch.rand(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape=torch.Size([4, 8, 8])\n",
      "(B, T, value_dims)=(4, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Project input tensor onto lower dimension\n",
    "query_projection = nn.Linear(C, retrieval_dims, bias=False)\n",
    "key_projection = nn.Linear(C, retrieval_dims, bias=False)\n",
    "value_projection = nn.Linear(C, value_dims, bias=False)\n",
    "\n",
    "query: torch.Tensor = query_projection(x)  # (B, T, retrieval_dims)\n",
    "key: torch.Tensor = key_projection(x)  # (B, T, retrieval_dims)\n",
    "value: torch.Tensor = value_projection(x)  # (B, T, value_dims)\n",
    "\n",
    "# Calculate attention_weight[i, j] = dot_product(query[i], key[j]) for each batch\n",
    "wei = query @ key.transpose(2, 1)  # (B, T, T) -> (T, T) = attention_weight\n",
    "\n",
    "# Scale attention weights to keep variance from exploding\n",
    "wei = wei * retrieval_dims**-0.5\n",
    "\n",
    "# Mask and normalize with softmax so each token only attends to previously tokens in the sequence\n",
    "mask = torch.tril(torch.ones(T, T)) == 0\n",
    "wei = wei.masked_fill(mask, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)  # (B, T, T) -> (T, T) = normalized_attention_weight\n",
    "\n",
    "# Weighted average of value tensors based on attention weights\n",
    "output = wei @ value  # (B, T, value_dims)\n",
    "print(f\"{output.shape=}\\n{(B, T, value_dims)=}\")\n",
    "\n",
    "# Sanity Checking\n",
    "assert torch.allclose(\n",
    "    output[:, 0], value[:, 0]\n",
    "), \"The first token should only tend to itself\"\n",
    "assert not torch.allclose(\n",
    "    output[:, 1:], value[:, 1:]\n",
    "), \"Future tokens attend to more than just themselves\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operate on 4 heads in parallel, each of which deal with 8 dimensional\n",
    "# keys, queries, and values\n",
    "heads = 4\n",
    "value_dims = retrieval_dims = C // heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project input to 8-d space for each head\n",
    "query_projection = nn.Linear(C, retrieval_dims * heads, bias=False)\n",
    "key_projection = nn.Linear(C, retrieval_dims * heads, bias=False)\n",
    "value_projection = nn.Linear(C, value_dims * heads, bias=False)\n",
    "\n",
    "query: torch.Tensor = query_projection(x)\n",
    "key: torch.Tensor = key_projection(x)\n",
    "value: torch.Tensor = value_projection(x)\n",
    "\n",
    "query = query.view(B, T, heads, retrieval_dims)\n",
    "key = key.view(B, T, heads, retrieval_dims)\n",
    "value = value.view(B, T, heads, value_dims)\n",
    "\n",
    "# Calculate attention weights\n",
    "wei = torch.einsum(\"bihk,bjhk->bhij\", query, key)  # (B, heads, T, T)\n",
    "mask = torch.tril(torch.ones((T, T))) == 0\n",
    "wei = wei.masked_fill(mask, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)  # (B, heads, T, T)\n",
    "\n",
    "# Calculate weighted value vectors\n",
    "# I wonder if there's a more efficient way to do this without needing to call .contiguous().\n",
    "# Does torch.compile take care of this for you?\n",
    "output = torch.einsum(\n",
    "    \"bhij,bjhc->bihc\", wei, value\n",
    ").contiguous()  # (B, T, heads, value_dims)\n",
    "\n",
    "# Concatenate value vectors\n",
    "output = output.view(B, T, heads * value_dims)\n",
    "\n",
    "# Sanity check\n",
    "assert torch.allclose(output[:, 0], value[:, 0].view(B, -1))\n",
    "assert not torch.allclose(output[:, 1:], value[:, 1:].view(B, T - 1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-to-hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
